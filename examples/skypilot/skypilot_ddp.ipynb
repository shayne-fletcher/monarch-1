{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Monarch DDP Example with SkyPilot\n",
        "\n",
        "This notebook demonstrates running PyTorch DDP (DistributedDataParallel) training on cloud infrastructure provisioned by SkyPilot.\n",
        "\n",
        "Adapted from the SLURM DDP example (`slurm_ddp.ipynb`).\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "```bash\n",
        "pip install torchmonarch-nightly\n",
        "pip install skypilot[kubernetes]  # or skypilot[aws], skypilot[gcp], etc.\n",
        "sky check  # Verify SkyPilot configuration\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set timeouts before importing monarch\n",
        "os.environ[\"HYPERACTOR_HOST_SPAWN_READY_TIMEOUT\"] = \"300s\"\n",
        "os.environ[\"HYPERACTOR_MESSAGE_DELIVERY_TIMEOUT\"] = \"300s\"\n",
        "os.environ[\"HYPERACTOR_MESH_PROC_SPAWN_MAX_IDLE\"] = \"300s\"\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import sky\n",
        "from monarch.actor import Actor, current_rank, endpoint\n",
        "from monarch.utils import setup_env_for_distributed\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# Import SkyPilotJob from local package\n",
        "from monarch_skypilot import SkyPilotJob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Model and DDP Actor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToyModel(nn.Module):\n",
        "    \"\"\"A simple toy model for demonstration purposes.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ToyModel, self).__init__()\n",
        "        self.net1 = nn.Linear(10, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.net2 = nn.Linear(10, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net2(self.relu(self.net1(x)))\n",
        "\n",
        "\n",
        "class DDPActor(Actor):\n",
        "    \"\"\"This Actor wraps the basic functionality from Torch's DDP example.\n",
        "\n",
        "    Adapted from: https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.rank = current_rank().rank\n",
        "\n",
        "    @endpoint\n",
        "    async def setup(self) -> str:\n",
        "        \"\"\"Initialize the PyTorch distributed process group.\"\"\"\n",
        "        WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"])\n",
        "        dist.init_process_group(\"gloo\", rank=self.rank, world_size=WORLD_SIZE)\n",
        "        return f\"Rank {self.rank}: Initialized distributed (world_size={WORLD_SIZE})\"\n",
        "\n",
        "    @endpoint\n",
        "    async def cleanup(self) -> str:\n",
        "        \"\"\"Clean up the PyTorch distributed process group.\"\"\"\n",
        "        dist.destroy_process_group()\n",
        "        return f\"Rank {self.rank}: Cleaned up distributed\"\n",
        "\n",
        "    @endpoint\n",
        "    async def demo_basic(self) -> str:\n",
        "        \"\"\"Run a basic DDP training example.\"\"\"\n",
        "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "        model = ToyModel().to(local_rank)\n",
        "        ddp_model = DDP(model, device_ids=[local_rank])\n",
        "\n",
        "        loss_fn = nn.MSELoss()\n",
        "        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = ddp_model(torch.randn(20, 10))\n",
        "        labels = torch.randn(20, 5).to(local_rank)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return f\"Rank {self.rank}: Training step complete (loss={loss.item():.4f})\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure your cloud provider, cluster size, and GPU type below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - modify these values as needed\n",
        "CLOUD = \"kubernetes\"  # Options: kubernetes, aws, gcp, azure\n",
        "NUM_HOSTS = 2\n",
        "GPUS_PER_HOST = 1\n",
        "CLUSTER_NAME = \"monarch-ddp\"\n",
        "ACCELERATOR = \"H200:1\"  # e.g., H100:1, A100:1, V100:1\n",
        "\n",
        "def get_cloud(cloud_name: str):\n",
        "    \"\"\"Get SkyPilot cloud object from name.\"\"\"\n",
        "    clouds = {\n",
        "        \"kubernetes\": sky.Kubernetes,\n",
        "        \"aws\": sky.AWS,\n",
        "        \"gcp\": sky.GCP,\n",
        "        \"azure\": sky.Azure,\n",
        "    }\n",
        "    if cloud_name.lower() not in clouds:\n",
        "        raise ValueError(f\"Unknown cloud: {cloud_name}. Available: {list(clouds.keys())}\")\n",
        "    return clouds[cloud_name.lower()]()\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Cloud: {CLOUD}\")\n",
        "print(f\"  Hosts: {NUM_HOSTS}\")\n",
        "print(f\"  GPUs per host: {GPUS_PER_HOST}\")\n",
        "print(f\"  Accelerator: {ACCELERATOR}\")\n",
        "print(f\"  Cluster name: {CLUSTER_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create SkyPilot Job\n",
        "\n",
        "Create a SkyPilot job to provision cloud instances:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job = SkyPilotJob(\n",
        "    meshes={\"mesh0\": NUM_HOSTS},\n",
        "    resources=sky.Resources(\n",
        "        cloud=get_cloud(CLOUD),\n",
        "        accelerators=ACCELERATOR,\n",
        "    ),\n",
        "    cluster_name=CLUSTER_NAME,\n",
        "    idle_minutes_to_autostop=10,\n",
        "    down_on_autostop=True,\n",
        ")\n",
        "\n",
        "print(f\"SkyPilot job created for cluster '{CLUSTER_NAME}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch Cluster and Create Process Mesh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the cluster and get the job state\n",
        "print(\"Launching SkyPilot cluster...\")\n",
        "job_state = job.state()\n",
        "\n",
        "# Create process mesh with GPUs\n",
        "print(\"Creating process mesh...\")\n",
        "proc_mesh = job_state.mesh0.spawn_procs({\"gpus\": GPUS_PER_HOST})\n",
        "print(f\"Process mesh extent: {proc_mesh.extent}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spawn DDP Actors and Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spawn DDP actors on the process mesh\n",
        "print(\"Spawning DDP actors...\")\n",
        "ddp_actor = proc_mesh.spawn(\"ddp_actor\", DDPActor)\n",
        "\n",
        "# Set up the distributed environment\n",
        "print(\"Setting up distributed environment...\")\n",
        "await setup_env_for_distributed(proc_mesh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the DDP example\n",
        "print(\"Running DDP training...\\n\")\n",
        "\n",
        "# Initialize distributed process group\n",
        "print(\"[1] Initializing distributed process group...\")\n",
        "results = await ddp_actor.setup.call()\n",
        "for coord, msg in results:\n",
        "    print(f\"    {msg}\")\n",
        "\n",
        "# Run the basic DDP training example\n",
        "print(\"\\n[2] Running DDP training step...\")\n",
        "results = await ddp_actor.demo_basic.call()\n",
        "for coord, msg in results:\n",
        "    print(f\"    {msg}\")\n",
        "\n",
        "# Clean up distributed process group\n",
        "print(\"\\n[3] Cleaning up distributed process group...\")\n",
        "results = await ddp_actor.cleanup.call()\n",
        "for coord, msg in results:\n",
        "    print(f\"    {msg}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DDP example completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Tear down the SkyPilot cluster when done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tear down the SkyPilot cluster\n",
        "print(\"Cleaning up SkyPilot cluster...\")\n",
        "job.kill()\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
