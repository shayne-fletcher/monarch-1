{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c91f6d2",
   "metadata": {},
   "source": [
    "## Monarch + TorchTitan on SLURM\n",
    "This example notebook demonstrates how you can easily run and iterate on a distributed training job with Monarch and TorchTitan.\n",
    "\n",
    "#### Prerequisites\n",
    "Please make sure your environment is setup for this notebook:\n",
    "1. Install Monarch nightly: https://github.com/meta-pytorch/monarch/blob/main/scripts/install_nightly.py\n",
    "2. Install Titan nightly: https://github.com/pytorch/torchtitan?tab=readme-ov-file#nightly-builds\n",
    "3. Ensure you have a valid Titan model config in the script directory (i.e: https://github.com/pytorch/torchtitan/blob/main/torchtitan/models/llama3/train_configs/debug_model.toml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd971d",
   "metadata": {},
   "source": [
    "### 1. Create your SLURM job\n",
    "Configure parameters for your cluster:\n",
    "- num_nodes: Number of nodes to allocate (default: 2)\n",
    "- gpus_per_node: Number of GPUs per node (default: 8)\n",
    "- mesh_name: Name for the mesh (default: \"mesh0\")\n",
    "- time_limit: Maximum job duration (default: \"06:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "import logging\n",
    "from monarch.job import SlurmJob\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(name)s %(asctime)s %(levelname)s %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    force=True,\n",
    ")\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure job parameters\n",
    "num_nodes = 2  # assign for your system\n",
    "gpus_per_node = 8  # adjust for your hardware\n",
    "mesh_name = \"mesh0\"\n",
    "\n",
    "# Create a SLURM job with N nodes\n",
    "slurm_job = SlurmJob(\n",
    "    meshes={mesh_name: num_nodes},\n",
    "    job_name=\"monarch_example\",\n",
    "    gpus_per_node=gpus_per_node,\n",
    "    time_limit=\"06:00:00\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e41ce",
   "metadata": {},
   "source": [
    "### 2. Define your Titan and cluster parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d51df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "from torchtitan.train import Trainer\n",
    "from torchtitan.config import ConfigManager, JobConfig\n",
    "from monarch.actor import Actor, current_rank, endpoint\n",
    "from torchtitan.tools.logging import init_logger, logger\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from monarch.utils import setup_env_for_distributed\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RunParams:\n",
    "    \"\"\"\n",
    "        Parameters for your cluster and training job, adjust as needed\n",
    "    \"\"\"\n",
    "    training_steps: int = 50\n",
    "    model_config = \"debug_model.toml\"\n",
    "    dataset = \"c4\"\n",
    "    num_nodes = num_nodes\n",
    "    gpus_per_node = gpus_per_node\n",
    "\n",
    "\n",
    "class TrainerActor(Actor):\n",
    "    \"\"\"\n",
    "        A simple wrapper class with executes a TorchTitan trainer in a Monarch actor\n",
    "    \"\"\"\n",
    "    def __init__(self, job_config: JobConfig) -> None:\n",
    "        self.job_config = job_config\n",
    "        rank = current_rank().rank\n",
    "        self.uid = f\"[trainer_{rank}]\"\n",
    "\n",
    "    @endpoint\n",
    "    async def start_training(self) -> None:\n",
    "        init_logger()\n",
    "        trainer: Trainer | None = None\n",
    "\n",
    "        try:\n",
    "            trainer = Trainer(self.job_config)\n",
    "            logger.info(f\"{self.uid} initialized successfully and starting training\")\n",
    "            trainer.train()\n",
    "        except Exception:\n",
    "            if trainer:\n",
    "                trainer.close()\n",
    "            raise\n",
    "        else:\n",
    "            trainer.close()\n",
    "        finally:\n",
    "            torch.distributed.destroy_process_group()\n",
    "            logger.info(f\"{self.uid} trainer cleaned up\")\n",
    "\n",
    "def make_job_config() -> JobConfig:\n",
    "    \"\"\"\n",
    "        Create a job config which is digested by TorchTitan, sourced from RunParams\n",
    "    \"\"\"\n",
    "    data_parallel_shard_degree = RunParams.num_nodes * RunParams.gpus_per_node\n",
    "    output_path = \"./outputs\"\n",
    "\n",
    "    script_dir = globals()['_dh'][0]\n",
    "    default_args = [\n",
    "        \"--job.config_file\",\n",
    "        os.path.join(script_dir, RunParams.model_config),\n",
    "        \"--model.tokenizer_path\",\n",
    "        os.path.join(script_dir, \"tokenizer\"),\n",
    "        \"--comm.trace_buf_size\",\n",
    "        \"0\",\n",
    "        \"--metrics.log_freq\",\n",
    "        \"1\",\n",
    "        \"--parallelism.data_parallel_shard_degree\",\n",
    "        str(data_parallel_shard_degree),\n",
    "        \"--activation_checkpoint.mode\",\n",
    "        \"full\",\n",
    "        \"--comm.train_timeout_seconds\",\n",
    "        \"60\",\n",
    "        \"--training.steps\",\n",
    "        str(RunParams.training_steps),\n",
    "        \"--training.dataset\",\n",
    "        RunParams.dataset,\n",
    "        \"--job.dump_folder\",\n",
    "        output_path,\n",
    "        \"--metrics.enable_tensorboard\",\n",
    "    ]\n",
    "\n",
    "    config_manager = ConfigManager()\n",
    "    job_config = config_manager.parse_args(default_args)\n",
    "\n",
    "    return job_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04425384",
   "metadata": {},
   "source": [
    "### 3. Execute your training job\n",
    "You can make adjustments and run this on the existing SLURM allocations as many times as you would like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "async def main():\n",
    "    job_config = make_job_config()\n",
    "\n",
    "    try:\n",
    "        # 1. Get job state and create process mesh\n",
    "        job_state = slurm_job.state()\n",
    "        proc_mesh = job_state.mesh0.spawn_procs({\"gpus\": RunParams.gpus_per_node})\n",
    "        \n",
    "        # 2. Configure remote logging behavior\n",
    "        await proc_mesh.logging_option(\n",
    "            stream_to_client=True,\n",
    "            # aggregate_window_sec=None  # Uncomment to disable log batching\n",
    "        )\n",
    "        \n",
    "        # 3. Setup environment for torch.distributed\n",
    "        await setup_env_for_distributed(proc_mesh)\n",
    "        \n",
    "        # 4. Spawn TrainerActor on each GPU\n",
    "        trainer = proc_mesh.spawn(\"trainer_actor\", TrainerActor, job_config)\n",
    "        \n",
    "        # 5. Execute the training job\n",
    "        await trainer.start_training.call()\n",
    "        \n",
    "        logger.info(\"Training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training workflow failed: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13bf71",
   "metadata": {},
   "source": [
    "### 4. Cleanup the SLURM job\n",
    "Once you're done experimenting, free up the allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.\n",
    "\n",
    "# Cancel the SLURM job, releasing all reserved nodes back to the cluster\n",
    "slurm_job.kill()\n",
    "logger.info(\"Job terminated successfully\")"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "2fa680ca-06ba-41e3-ac40-90b22d77bbc3",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python (monarch)",
   "language": "python",
   "name": "monarch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
